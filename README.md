# transformer_exercise

In this Jupyter Notebook, I show my code and some of the notes I used when trying to code a transformer model from scratch. This was an exercise to really understand every part of a transformer architecture and how they interact. I found learning about positional encoding very interesting and the exercise really solidified my understanding of how attention heads compute attention in parallel and combine information at the end for the sake of efficiency.
