{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bbab28",
   "metadata": {},
   "source": [
    "# Transformer Coded from Scratch\n",
    "\n",
    "In this Jupyter Notebook, I show my code and some of the notes I used when trying to code a transformer model from scratch. This was an exercise to really understand every part of a transformer architecture and how they interact. I found learning about positional encoding very interesting and the exercise really solidified my understanding of how attention heads compute attention in parallel and combine information at the end for the sake of efficiency. \n",
    "\n",
    "The code was quite buggy, naming conventions are misused and I replaced some functions while debugging (commenting out or deleting the origional functions). However, I learnt not to do this as many issues in the code were not to do with the function where the error was but occurred before that and were only flagged later. I enjoyed fixing the code and reflecting on where I went wrong in this process and will use the lessons I have learnt, mainly consistently assigning variables and properly naming objects (sloppy mistakes I know). \n",
    "\n",
    "## So take this code with a pinch of salt. It is not something I would present to someone else, I have much higher standards. Take it as proof that an exercise has been completed and learning has been done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097bbc9",
   "metadata": {},
   "source": [
    "What are the things i need:\n",
    "\n",
    "TRansformer strucutre: \n",
    "\n",
    "Masking for source and target\n",
    "\n",
    "embedding layer, positional embedding, mask and then funnel it through the encoder?\n",
    "\n",
    "decoder\n",
    "\n",
    "final layer you either softmax of the vocab to find the next item in the sequence or you put it through some more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcaa0a3",
   "metadata": {},
   "source": [
    "Masking\n",
    "it needs a matrix of the number of sequences x the longest equence length\n",
    "it needs a way to change all the padding tokens to 0 and it needs a boolen method to turn that into true and false\n",
    "\n",
    "\n",
    "\n",
    "class Mak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3603db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "197e80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_len):\n",
    "        super(PositionEncoding, self).__init__()\n",
    "\n",
    "        #matrix of positional encodings\n",
    "        pe = torch.zeros(max_sequence_len, d_model)\n",
    "        position = torch.arange(0, max_sequence_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        #unsqueeze adds a dimension to make it suitable as a module - extra layer is for batching although\n",
    "        #that is not happening here\n",
    "        self.register_buffer('pe', pe)    \n",
    "    #model buffers are not considered part of the model paramteres and are not updates in training\n",
    "\n",
    "    def forward(self, x):\n",
    "        #add pos encodings to te input\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69f432d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionWiseSelfAttention(nn.Module):\n",
    "\n",
    "    #with attention what do i need to do - split heads, calculate attetion and multiply it to v? \n",
    "    #then combine heads again and output? isn't there a linear transformation at the end\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(PositionWiseSelfAttention, self).__init__()\n",
    "\n",
    "\n",
    "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def scaled_dot_product(self, Q, K, V, mask=None):\n",
    "        attn_scores =  torch.matmul(Q, K.transpose(-2, -1)/math.sqrt(self.d_k))\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        att_output = self.scaled_dot_product(Q, K, V, mask)\n",
    "\n",
    "        output = self.W_o(self.combine_heads(att_output))\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "055a3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_big):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_big)\n",
    "        self.ReLu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(d_big, d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.ReLu(self.linear_1(x)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a56da59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.multi_head_attn = PositionWiseSelfAttention(d_model, num_heads)\n",
    "        self.ff = PositionWiseFeedForward(d_model, d_big=d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        step_one = self.multi_head_attn(x, x, x, mask)\n",
    "        step_two = self.norm_1(x +self.dropout(step_one))\n",
    "        step_3 = self.ff(step_two)\n",
    "        step_4 = self.norm_2(step_one + self.dropout(step_3))\n",
    "\n",
    "        return step_4'''\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = PositionWiseSelfAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9bf9157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.self_attn = PositionWiseSelfAttention(d_model, num_heads)\n",
    "        self.cross_attn = PositionWiseSelfAttention(d_model, num_heads)\n",
    "        self.ff = PositionWiseFeedForward(d_model, d_big=d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.droput = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_out, source_mask, target_mask):\n",
    "        self_attention = self.self_attn(x, x, x, target_mask)\n",
    "        norm_1 = self.norm1(x + self.droput(self_attention))\n",
    "\n",
    "        cross_attention = self.cross_attn(x, enc_out, enc_out, source_mask)\n",
    "\n",
    "        norm_2 = (norm_1 + self.droput(cross_attention))\n",
    "        feedforward = self.ff(norm_2)\n",
    "        norm_3 = (norm_2 + self.droput(feedforward))\n",
    "\n",
    "        return norm_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cf02db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def forward(self, source, target):\\n        source_mask, target_mask = self.create_masks(source, target)\\n\\n        enc_bed = self.encoder_embedding(source)\\n        position_n_inbed = self.dropout(self.positional_encoding(enc_bed))\\n\\n        encoding = position_n_inbed\\n        for layer in self.encoder_layers:\\n            encoding = layer(encoding, source_mask)\\n\\n        dec_bed = self.decoder_embedding(target)\\n        dec_pos = self.dropout(self.positional_encoding(dec_bed))\\n\\n        decoding = dec_pos\\n        for layer in self.decoder_layers:\\n            print(source_mask.size(), target_mask.size())\\n            decoding = layer(decoding, encoding, source_mask, target_mask)\\n\\n        output = self.fc(decoding)\\n        return output'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correct format to generate mask layers\n",
    "# don't need a whole class - you are not maniputating or re-using the mask layers so you can chill\n",
    "# part of transformer put together but you could probably write it by itself\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "'''class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()  # Ensure this is the first line in the __init__ method\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([Decoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "\n",
    "    def create_masks( self, source, target):\n",
    "        #create boolean matrix and for every element, if it is 0 put false and non 0 put triue\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (source != 0).unsqueeze(1).unsqueeze(3)\n",
    "\n",
    "        sequence_len =target.size(1)\n",
    "\n",
    "        matrix_ones = torch.ones(1, sequence_len, sequence_len)\n",
    "        upper_triangle_matrix = torch.triu(matrix_ones, diagonal = 1)\n",
    "        #invert the matrix using some simple math - nice\n",
    "        lower_triangle = 1- upper_triangle_matrix\n",
    "        nopeak_mask = lower_triangle.bool()\n",
    "        target_mask = nopeak_mask & target_mask\n",
    "\n",
    "        return target_mask, source_mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        source_mask, target_mask = self.create_masks(x, x)\n",
    "\n",
    "        enc_bed = self.encoder_embedding(x)\n",
    "        position_n_inbed = self.dropout(self.positional_encoding(enc_bed))\n",
    "\n",
    "        encoding = position_n_inbed\n",
    "        for layer in self.encoderlayers:\n",
    "            encoding = layer(encoding, source_mask)\n",
    "\n",
    "        dec_bed = self.decoder_embedding(x)\n",
    "\n",
    "        dec_pos = self.dropout(self.positional_encoding(dec_bed))\n",
    "        \n",
    "        decoding = dec_pos\n",
    "        for layer in self.decoder_layers:\n",
    "\n",
    "            decoding = layer(decoding, encoding, source_mask, target_mask)\n",
    "\n",
    "        output = self.final_linear(decoding)\n",
    "\n",
    "        return output'''\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()  # Ensure this is the first line in the __init__ method\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([Decoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_masks(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source_mask, target_mask = self.create_masks(source, target)\n",
    "        \n",
    "        enc_bed = self.encoder_embedding(source)\n",
    "        position_n_inbed = self.dropout(self.positional_encoding(enc_bed))\n",
    "\n",
    "        encoding = position_n_inbed\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            encoding = layer(encoding, source_mask)\n",
    "\n",
    "        dec_bed = self.decoder_embedding(target)\n",
    "        dec_pos = self.dropout(self.positional_encoding(dec_bed))\n",
    "\n",
    "        decoding = dec_pos\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            decoding = layer(decoding, encoding, source_mask, target_mask)\n",
    "\n",
    "        output = self.fc(decoding)\n",
    "        return output\n",
    "\n",
    "''' def forward(self, source, target):\n",
    "        source_mask, target_mask = self.create_masks(source, target)\n",
    "\n",
    "        enc_bed = self.encoder_embedding(source)\n",
    "        position_n_inbed = self.dropout(self.positional_encoding(enc_bed))\n",
    "\n",
    "        encoding = position_n_inbed\n",
    "        for layer in self.encoder_layers:\n",
    "            encoding = layer(encoding, source_mask)\n",
    "\n",
    "        dec_bed = self.decoder_embedding(target)\n",
    "        dec_pos = self.dropout(self.positional_encoding(dec_bed))\n",
    "\n",
    "        decoding = dec_pos\n",
    "        for layer in self.decoder_layers:\n",
    "            print(source_mask.size(), target_mask.size())\n",
    "            decoding = layer(decoding, encoding, source_mask, target_mask)\n",
    "\n",
    "        output = self.fc(decoding)\n",
    "        return output'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60def8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Assuming you have a 'Transformer' class and necessary helper modules (like PositionalEncoding) defined elsewhere.\n",
    "\n",
    "# Hyperparameters (these define the size and structure of your Transformer model)\n",
    "src_vocab_size = 5000  # Size of the source vocabulary (e.g., number of unique characters/tokens in expressions)\n",
    "tgt_vocab_size = 5000  # Size of the target vocabulary (e.g., number of unique characters/tokens in results)\n",
    "d_model = 512         # Dimensionality of the embedding layer and the internal representations of the Transformer\n",
    "num_heads = 8         # Number of attention heads in the multi-head attention mechanism\n",
    "num_layers = 6        # Number of encoder (and potentially decoder) layers in the Transformer\n",
    "d_ff = 2048         # Dimensionality of the feed-forward network within each Transformer layer\n",
    "max_seq_length = 100  # Maximum length of the input and output sequences that the model can handle\n",
    "dropout = 0.1         # Dropout probability for regularization\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "# This creates an instance of your Transformer class with the specified hyperparameters.\n",
    "# It initializes all the layers and parameters of the model.\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# --- Process to Create Arithmetic Expression Data ---\n",
    "batch_size = 64  # Number of arithmetic expressions to generate in this batch\n",
    "max_len = max_seq_length  # Using the defined max sequence length for padding\n",
    "\n",
    "def generate_arithmetic_expressions(num_samples, max_length):\n",
    "    \"\"\"\n",
    "    Generates a list of simple arithmetic expressions and their results.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): The number of expression-result pairs to generate.\n",
    "        max_length (int): The maximum length of the expression and result sequences (for padding).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "              - a list of tokens representing the arithmetic expression\n",
    "              - a list of tokens representing the result\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    operators = [\"+\", \"-\"]\n",
    "    digits = \"0123456789\"\n",
    "    for _ in range(num_samples):\n",
    "        num1 = random.randint(1, 9)\n",
    "        num2 = random.randint(1, 9)\n",
    "        op = random.choice(operators)\n",
    "        expression = f\"{num1} {op} {num2}\"\n",
    "        try:\n",
    "            result = str(eval(expression))\n",
    "            src_sequence = list(expression)  # Split the expression into characters (tokens)\n",
    "            tgt_sequence = list(result)      # Split the result into characters (tokens)\n",
    "\n",
    "            # Pad sequences to max_length with a '<pad>' token (assuming you'll handle this in your vocabulary)\n",
    "            src_sequence = src_sequence + ['<pad>'] * (max_length - len(src_sequence))\n",
    "            tgt_sequence = tgt_sequence + ['<pad>'] * (max_length - len(tgt_sequence))\n",
    "\n",
    "            data.append((src_sequence, tgt_sequence))\n",
    "        except:\n",
    "            continue  # Skip if there's an error in evaluation (shouldn't happen with simple +/-)\n",
    "    return data\n",
    "\n",
    "# Generate the arithmetic dataset\n",
    "arithmetic_data = generate_arithmetic_expressions(batch_size, max_len)\n",
    "\n",
    "# --- Process to Convert Text Data to Numerical Tensors ---\n",
    "# You'll need to have a vocabulary (mapping from tokens to numbers) for both the source and target.\n",
    "# Assuming you have 'src_vocab' and 'tgt_vocab' dictionaries created elsewhere.\n",
    "\n",
    "def numericalize_sequence(sequence, vocab, max_length):\n",
    "    \"\"\"\n",
    "    Converts a sequence of tokens into a list of numerical indices based on the vocabulary.\n",
    "    Pads the sequence to the maximum length.\n",
    "\n",
    "    Args:\n",
    "        sequence (list): A list of tokens (e.g., characters).\n",
    "        vocab (dict): A dictionary mapping tokens to their numerical indices.\n",
    "        max_length (int): The maximum length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A 1D tensor of numerical indices.\n",
    "    \"\"\"\n",
    "    numericalized = [vocab.get(token, vocab.get('<unk>', 0)) for token in sequence] # Use <unk> if token not in vocab\n",
    "    padded = numericalized + [vocab.get('<pad>', 0)] * (max_length - len(numericalized))\n",
    "    return torch.tensor(padded)\n",
    "\n",
    "# Assuming you have built your source and target vocabularies ('src_vocab' and 'tgt_vocab')\n",
    "# based on the characters present in your arithmetic expressions and results.\n",
    "\n",
    "# Example vocabulary creation (you might have a more sophisticated way of doing this)\n",
    "all_chars = set()\n",
    "for src, tgt in arithmetic_data:\n",
    "    all_chars.update(src)\n",
    "    all_chars.update(tgt)\n",
    "all_chars.add('<pad>')\n",
    "all_chars = sorted(list(all_chars))\n",
    "char_to_index = {char: i for i, char in enumerate(all_chars)}\n",
    "src_vocab = char_to_index\n",
    "tgt_vocab = char_to_index # In this simple case, source and target vocab can be the same\n",
    "\n",
    "# Convert the generated arithmetic data into numerical tensors\n",
    "src_data = torch.stack([numericalize_sequence(item[0], src_vocab, max_len) for item in arithmetic_data])\n",
    "tgt_data = torch.stack([numericalize_sequence(item[1], tgt_vocab, max_len) for item in arithmetic_data])\n",
    "\n",
    "# Now 'src_data' and 'tgt_data' are PyTorch tensors containing the numerical representations\n",
    "# of your arithmetic expressions and their corresponding results, ready to be used for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c3d7bce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m output = transformer(src_data, tgt_data[:, :-\u001b[32m1\u001b[39m])\n\u001b[32m      9\u001b[39m loss = criterion(output.contiguous().view(-\u001b[32m1\u001b[39m, tgt_vocab_size), tgt_data[:, \u001b[32m1\u001b[39m:].contiguous().view(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m optimizer.step()\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b50d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 7 + 8, Predicted: , True: 15\n",
      "Input: 2 + 6, Predicted: , True: 8\n",
      "Input: 8 - 4, Predicted: , True: 4\n",
      "Input: 4 - 7, Predicted: , True: -3\n",
      "Input: 4 + 7, Predicted: , True: 11\n",
      "Input: 2 + 1, Predicted: , True: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Inference loop to generate the target sequence\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_seq_length):  \u001b[38;5;66;03m# Limit the generation length\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     output = \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass both src and tgt\u001b[39;00m\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Get the last predicted token\u001b[39;00m\n\u001b[32m     55\u001b[39m     predicted_index = torch.argmax(output[:, -\u001b[32m1\u001b[39m, :], dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, source, target)\u001b[39m\n\u001b[32m    100\u001b[39m decoding = dec_pos\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.decoder_layers):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     decoding = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m output = \u001b[38;5;28mself\u001b[39m.fc(decoding)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, x, enc_out, source_mask, target_mask)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, enc_out, source_mask, target_mask):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     self_attention = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     norm_1 = \u001b[38;5;28mself\u001b[39m.norm1(x + \u001b[38;5;28mself\u001b[39m.droput(self_attention))\n\u001b[32m     19\u001b[39m     cross_attention = \u001b[38;5;28mself\u001b[39m.cross_attn(x, enc_out, enc_out, source_mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mPositionWiseSelfAttention.forward\u001b[39m\u001b[34m(self, Q, K, V, mask)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, Q, K, V, mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     Q = \u001b[38;5;28mself\u001b[39m.split_heads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     54\u001b[39m     K = \u001b[38;5;28mself\u001b[39m.split_heads(\u001b[38;5;28mself\u001b[39m.W_k(K))\n\u001b[32m     55\u001b[39m     V = \u001b[38;5;28mself\u001b[39m.split_heads(\u001b[38;5;28mself\u001b[39m.W_v(V))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure your Transformer model is in evaluation mode\n",
    "transformer.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer.to(device)\n",
    "\n",
    "# Reuse the data generation function to create a validation set\n",
    "def generate_arithmetic_expressions(num_samples=32, max_length=max_seq_length): # Smaller validation set\n",
    "    data = []\n",
    "    operators = [\"+\", \"-\"]\n",
    "    digits = \"0123456789\"\n",
    "    for _ in range(num_samples):\n",
    "        num1 = random.randint(1, 9)\n",
    "        num2 = random.randint(1, 9)\n",
    "        op = random.choice(operators)\n",
    "        expression = f\"{num1} {op} {num2}\"\n",
    "        try:\n",
    "            result = str(eval(expression))\n",
    "            src_sequence = list(expression)\n",
    "            tgt_sequence = list(result)\n",
    "            src_sequence = src_sequence + ['<pad>'] * (max_length - len(src_sequence))\n",
    "            tgt_sequence = tgt_sequence + ['<pad>'] * (max_length - len(tgt_sequence))\n",
    "            data.append((src_sequence, tgt_sequence))\n",
    "        except:\n",
    "            continue\n",
    "    return data\n",
    "\n",
    "# Reuse the numericalization function\n",
    "def numericalize_sequence(sequence, vocab, max_length):\n",
    "    numericalized = [vocab.get(token, vocab.get('<unk>', 0)) for token in sequence]\n",
    "    padded = numericalized + [vocab.get('<pad>', 0)] * (max_length - len(numericalized))\n",
    "    return torch.tensor(padded).unsqueeze(0).to(device) # Add batch dimension\n",
    "\n",
    "# Generate the validation dataset\n",
    "validation_data = generate_arithmetic_expressions(num_samples=32)\n",
    "\n",
    "# Evaluation loop\n",
    "correct_predictions = 0\n",
    "total_samples = len(validation_data)\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation during evaluation\n",
    "    for src_text, true_tgt_text in validation_data:\n",
    "        src_tensor = numericalize_sequence(src_text, src_vocab, max_seq_length)\n",
    "\n",
    "        # **CORRECTION STARTS HERE**\n",
    "        # Create a target tensor for inference (start with a <start> token)\n",
    "        # Replace '<start>' with your actual start token if different\n",
    "        tgt_tensor = torch.tensor([tgt_vocab.get('<start>', 0)]).unsqueeze(0).to(device)\n",
    "\n",
    "        # Inference loop to generate the target sequence\n",
    "        for _ in range(max_seq_length):  # Limit the generation length\n",
    "            output = transformer(src_tensor, tgt_tensor)  # Pass both src and tgt\n",
    "            # Get the last predicted token\n",
    "            predicted_index = torch.argmax(output[:, -1, :], dim=-1)\n",
    "            # Append the predicted token to the target sequence\n",
    "            tgt_tensor = torch.cat([tgt_tensor, predicted_index.unsqueeze(0)], dim=1)\n",
    "            # Stop if the predicted token is the <end> token\n",
    "            # Replace '<end>' with your actual end token if different\n",
    "            if predicted_index.item() == tgt_vocab.get('<end>', 0):\n",
    "                break\n",
    "        # **CORRECTION ENDS HERE**\n",
    "\n",
    "        # Get the predicted sequence (argmax over the vocabulary dimension)\n",
    "        predicted_indices = torch.argmax(output, dim=-1).squeeze(0).cpu().numpy()\n",
    "\n",
    "        # Convert predicted indices back to tokens\n",
    "        index_to_tgt_char = {i: char for char, i in tgt_vocab.items()}\n",
    "        predicted_tokens = [index_to_tgt_char.get(idx, '<unk>') for idx in predicted_indices]\n",
    "\n",
    "        # Stop prediction at the first padding token\n",
    "        if '<pad>' in predicted_tokens:\n",
    "            predicted_tokens = predicted_tokens[:predicted_tokens.index('<pad>')]\n",
    "\n",
    "        # Stop true target at the first padding token\n",
    "        if '<pad>' in true_tgt_text:\n",
    "            true_tgt_text = true_tgt_text[:true_tgt_text.index('<pad>')]\n",
    "\n",
    "        predicted_result = \"\".join(predicted_tokens)\n",
    "        true_result = \"\".join(true_tgt_text)\n",
    "\n",
    "        print(f\"Input: {''.join(src_text).replace('<pad>', '')}, Predicted: {predicted_result}, True: {true_result}\")\n",
    "\n",
    "        if predicted_result == true_result:\n",
    "            correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"\\nValidation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
